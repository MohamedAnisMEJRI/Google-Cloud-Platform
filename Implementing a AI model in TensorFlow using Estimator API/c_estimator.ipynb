{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing tf.estimator\n",
    "\n",
    "**Learning Objectives**\n",
    "  - Understand where the `tf.estimator` module sits in the hierarchy of Tensorflow APIs\n",
    "  - Understand the workflow of creating a `tf.estimator` model\n",
    "    1. Create Feature Columns\n",
    "    2. Create Input Functions\n",
    "    3. Create Estimator\n",
    "    4. Train/Evaluate/Predict\n",
    "  - Understand how to swap in/out different types of Estimators\n",
    "  \n",
    "## Introduction\n",
    "\n",
    "Tensorflow is a hierarchical framework. The further down the hierarchy you go, the more flexibility you have, but that more code you have to write. Generally one starts at the highest level of abstraction. Then if you need additional flexibility drop down one layer.\n",
    "\n",
    "<img src='assets/TFHierarchy.png' width='50%'>\n",
    "<sup>(image: https://www.tensorflow.org/guide/premade_estimators)</sup>\n",
    "\n",
    "In this notebook we will be operating at the highest level of Tensorflow abstraction, using the Estimator API to predict taxifare prices on the sampled dataset we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.13.1\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/f2/0931c194bb98398017d52c94ee30e5e1a4082ab6af76e204856ff1fdb33e/tensorflow-1.13.1-cp35-cp35m-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 92.5MB 13kB/s \n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/7e/8e/9e446349fc449951ecf3768070483ea88e76725cdd5bbddb9bc50f6948d4/grpcio-1.22.0-cp35-cp35m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.2MB 689kB/s \n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 3.6MB/s \n",
      "\u001b[?25hCollecting wheel>=0.26 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/10/44230dd6bf3563b8f227dbf344c908d412ad2ff48066476672f3a72e174e/wheel-0.33.4-py2.py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 12.3MB/s \n",
      "\u001b[?25hCollecting protobuf>=3.6.1 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/55/34/7158a5ec978f12307eb361a8c4fdd867a8e2a0ab63fac99e5f555ee796d2/protobuf-3.9.0-cp35-cp35m-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 1.2MB/s \n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 478kB/s \n",
      "\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 7.5MB/s \n",
      "\u001b[?25hCollecting keras-applications>=1.0.6 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 10.6MB/s \n",
      "\u001b[?25hCollecting numpy>=1.13.3 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/69/25/eef8d362bd216b11e7d005331a3cca3d19b0aa57569bde680070109b745c/numpy-1.17.0-cp35-cp35m-manylinux1_x86_64.whl (20.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.2MB 70kB/s \n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting setuptools (from protobuf>=3.6.1->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl (575kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 2.5MB/s \n",
      "\u001b[?25hCollecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/ab/d3bed6b92042622d24decc7aadc8877badf18aeca1571045840ad4956d3f/Werkzeug-0.15.5-py2.py3-none-any.whl (328kB)\n",
      "\u001b[K    100% |████████████████████████████████| 337kB 4.2MB/s \n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 10.7MB/s \n",
      "\u001b[?25hCollecting h5py (from keras-applications>=1.0.6->tensorflow==1.13.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/77/c4933e12dca0f61bcdafc207c7532e1250b8d12719459fd85132f3daa9fd/h5py-2.9.0-cp35-cp35m-manylinux1_x86_64.whl (2.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.8MB 519kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: gast, absl-py, termcolor\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built gast absl-py termcolor\n",
      "Installing collected packages: six, grpcio, gast, absl-py, mock, numpy, tensorflow-estimator, wheel, keras-preprocessing, setuptools, protobuf, astor, werkzeug, markdown, tensorboard, h5py, keras-applications, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 grpcio-1.22.0 h5py-2.9.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 mock-3.0.5 numpy-1.17.0 protobuf-3.9.0 setuptools-41.0.1 six-1.12.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 werkzeug-0.15.5 wheel-0.33.4\n"
     ]
    }
   ],
   "source": [
    "# Ensure that we have Tensorflow 1.13.1 installed.\n",
    "!pip3 freeze | grep tensorflow==1.13.1 || pip3 install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data \n",
    "\n",
    "First, let's download the raw .csv data. These are the same files created in the `create_datasets.ipynb` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/taxifare/small/taxi-test.csv...\n",
      "Copying gs://cloud-training-demos/taxifare/small/taxi-train.csv...\n",
      "Copying gs://cloud-training-demos/taxifare/small/taxi-valid.csv...\n",
      "/ [3 files][ 10.9 MiB/ 10.9 MiB]                                                \n",
      "Operation completed over 3 objects/10.9 MiB.                                     \n",
      "-rw-r--r-- 1 jupyter jupyter 1799474 Jul 29 14:28 taxi-test.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 7986353 Jul 29 14:28 taxi-train.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 1673742 Jul 29 14:28 taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-training-demos/taxifare/small/*.csv .\n",
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the files are small we can load them into in-memory Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fare_amount', 'dayofweek', 'hourofday', 'pickuplon', 'pickuplat', 'dropofflon', 'dropofflat']\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(filepath_or_buffer = \"./taxi-train.csv\")\n",
    "df_valid = pd.read_csv(filepath_or_buffer = \"./taxi-valid.csv\")\n",
    "df_test = pd.read_csv(filepath_or_buffer = \"./taxi-test.csv\")\n",
    "\n",
    "CSV_COLUMN_NAMES = list(df_train)\n",
    "print(CSV_COLUMN_NAMES)\n",
    "\n",
    "FEATURE_NAMES = CSV_COLUMN_NAMES[1:] # all but first column\n",
    "LABEL_NAME = CSV_COLUMN_NAMES[0] # first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature columns\n",
    "\n",
    "Feature columns make it easy to perform common type of feature engineering on your raw data. For example you can one-hot encode categorical data, create feature crosses, embeddings and more. We'll cover these later in the course, but if you want to a sneak peak browse the official TensorFlow [feature columns guide](https://www.tensorflow.org/guide/feature_columns).\n",
    "\n",
    "In our case we won't do any feature engineering. However we still need to create a list of feature columns because the Estimator we will use requires one. To specify the numeric values should be passed on without modification we use `tf.feature_column.numeric_column()`\n",
    "\n",
    "We use a [python list comprehension](https://www.pythonforbeginners.com/basics/list-comprehensions-in-python) to create the list of feature columns, which is just an elegant alternative to a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='dayofweek', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='hourofday', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='pickuplon', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='pickuplat', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='dropofflon', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='dropofflat', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(key = k) for k in FEATURE_NAMES]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input function\n",
    "\n",
    "Now that your estimator knows what type of data to expect and how to intepret it, you need to actually pass the data to it! This is the job of the input function.\n",
    "\n",
    "The input function returns a new batch of (features, label) tuples each time it is called by the Estimator.\n",
    "\n",
    "- features: A python dictionary. Each key is a feature column name and its value is the tensor containing the data for that feature\n",
    "- label: A Tensor containing the labels\n",
    "\n",
    "So how do we get from our current Pandas dataframes to (features, label) tuples that return one batch at a time?\n",
    "\n",
    "The `tf.data` module contains a collection of classes that allows you to easily load data, manipulate it, and pipe it into your model. https://www.tensorflow.org/guide/datasets_for_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(df, batch_size = 128):\n",
    "    #1. Convert dataframe into correct (features,label) format for Estimator API\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tensors = (dict(df[FEATURE_NAMES]), df[LABEL_NAME]))\n",
    "    \n",
    "    # Note:\n",
    "    # If we returned now, the Dataset would iterate over the data once  \n",
    "    # in a fixed order, and only produce a single element at a time.\n",
    "    \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(buffer_size = 1000).repeat(count = None).batch(batch_size = batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(df, batch_size = 128):\n",
    "    #1. Convert dataframe into correct (features,label) format for Estimator API\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tensors = (dict(df[FEATURE_NAMES]), df[LABEL_NAME]))\n",
    "    \n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def predict_input_fn(df, batch_size = 128):\n",
    "    #1. Convert dataframe into correct (features) format for Estimator API\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tensors = dict(df[FEATURE_NAMES])) # no label\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Estimator\n",
    "\n",
    "Tensorflow has several premade estimators for you to choose from:\n",
    "\n",
    "- LinearClassifier/Regressor\n",
    "- BoostedTreesClassifier/Regressor\n",
    "- DNNClassifier/Regressor\n",
    "- DNNLinearCombinedClassifier/Regressor\n",
    "\n",
    "If none of these meet your needs you can implement a custom estimator using `tf.Keras`. We'll cover that later in the course.\n",
    "\n",
    "For now we will use the premade LinearRegressor. To instantiate an estimator simply pass it what feature columns to expect and specify an directory for it to output checkpoint files to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = \"taxi_trained\"\n",
    "\n",
    "model = tf.estimator.LinearRegressor(\n",
    "    feature_columns = feature_columns,\n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(tf_random_seed = 1) # for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Simply invoke the estimator's `train()` function. Specify the `input_fn` which tells it how to load in data, and specify the number of steps to train for.\n",
    "\n",
    "By default estimators check the output directory for checkpoint files before beginning training, so it can pickup where it last left off. To prevent this we'll delete the output directory before starting training each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 14:28:17.316154 139729986717440 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0729 14:28:17.377522 139729986717440 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "I0729 14:28:17.395714 139729986717440 estimator.py:1145] Calling model_fn.\n",
      "W0729 14:28:17.972416 139729986717440 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/canned/linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "I0729 14:28:18.299417 139729986717440 estimator.py:1147] Done calling model_fn.\n",
      "I0729 14:28:18.301167 139729986717440 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0729 14:28:18.657573 139729986717440 monitored_session.py:240] Graph was finalized.\n",
      "I0729 14:28:18.767182 139729986717440 session_manager.py:500] Running local_init_op.\n",
      "I0729 14:28:18.779609 139729986717440 session_manager.py:502] Done running local_init_op.\n",
      "I0729 14:28:20.694743 139729986717440 basic_session_run_hooks.py:606] Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
      "I0729 14:28:21.418909 139729986717440 basic_session_run_hooks.py:262] loss = 22720.79, step = 1\n",
      "I0729 14:28:21.910571 139729986717440 basic_session_run_hooks.py:692] global_step/sec: 203.207\n",
      "I0729 14:28:21.913205 139729986717440 basic_session_run_hooks.py:260] loss = 6976.6094, step = 101 (0.494 sec)\n",
      "I0729 14:28:22.225769 139729986717440 basic_session_run_hooks.py:692] global_step/sec: 317.257\n",
      "I0729 14:28:22.228132 139729986717440 basic_session_run_hooks.py:260] loss = 14335.657, step = 201 (0.315 sec)\n",
      "I0729 14:28:22.534693 139729986717440 basic_session_run_hooks.py:692] global_step/sec: 323.688\n",
      "I0729 14:28:22.538601 139729986717440 basic_session_run_hooks.py:260] loss = 8976.088, step = 301 (0.310 sec)\n",
      "I0729 14:28:22.838982 139729986717440 basic_session_run_hooks.py:692] global_step/sec: 328.642\n",
      "I0729 14:28:22.843631 139729986717440 basic_session_run_hooks.py:260] loss = 20098.215, step = 401 (0.305 sec)\n",
      "I0729 14:28:23.208994 139729986717440 basic_session_run_hooks.py:606] Saving checkpoints for 500 into taxi_trained/model.ckpt.\n",
      "I0729 14:28:23.322766 139729986717440 estimator.py:368] Loss for final step: 13854.729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.78 s, sys: 2.82 s, total: 11.6 s\n",
      "Wall time: 6.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressor at 0x7f150a982978>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "model.train(\n",
    "    input_fn = lambda: train_input_fn(df = df_train), \n",
    "    steps = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Estimators similarly have an `evaluate()` function. In this case we don't need to specify the number of steps to train because we didn't tell our input function to repeat the data. Once the input function reaches the end of the data evaluation will end. \n",
    "\n",
    "Loss is reported as MSE by default so we take the square root before printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 14:28:23.392342 139729986717440 estimator.py:1145] Calling model_fn.\n",
      "I0729 14:28:24.022541 139729986717440 estimator.py:1147] Done calling model_fn.\n",
      "I0729 14:28:24.046400 139729986717440 evaluation.py:255] Starting evaluation at 2019-07-29T14:28:24Z\n",
      "I0729 14:28:24.166563 139729986717440 monitored_session.py:240] Graph was finalized.\n",
      "W0729 14:28:24.168464 139729986717440 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "I0729 14:28:24.170772 139729986717440 saver.py:1280] Restoring parameters from taxi_trained/model.ckpt-500\n",
      "I0729 14:28:24.235679 139729986717440 session_manager.py:500] Running local_init_op.\n",
      "I0729 14:28:24.267910 139729986717440 session_manager.py:502] Done running local_init_op.\n",
      "I0729 14:28:25.530293 139729986717440 evaluation.py:275] Finished evaluation at 2019-07-29-14:28:25\n",
      "I0729 14:28:25.531972 139729986717440 estimator.py:2039] Saving dict for global step 500: average_loss = 89.48336, global_step = 500, label/mean = 11.229713, loss = 11435.183, prediction/mean = 12.608461\n",
      "I0729 14:28:25.636353 139729986717440 estimator.py:2099] Saving 'checkpoint_path' summary for global step 500: taxi_trained/model.ckpt-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on dataset = 9.459564487360257\n"
     ]
    }
   ],
   "source": [
    "def print_rmse(model, df):\n",
    "    metrics = model.evaluate(input_fn = lambda: eval_input_fn(df))\n",
    "    print(\"RMSE on dataset = {}\".format(metrics[\"average_loss\"]**.5))\n",
    "print_rmse(model = model, df = df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE of 9.45 is  worse than our rules based benchmark (RMSE of $7.70). However given that we haven't done any feature engineering or hyperparameter tuning, and we're training on a small dataset using a simple linear model, we shouldn't yet expect good performance. \n",
    "\n",
    "The goal at this point is to demonstrate the mechanics of the Estimator API. In subsequent notebooks we'll improve on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "To make predictions with our trained model, we call the `predict` method of our estimator, passing to it the input function to read in the data. Here we make predictions for the first 10 elements of the `df_test` dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 14:28:25.669340 139729986717440 estimator.py:1145] Calling model_fn.\n",
      "I0729 14:28:25.965878 139729986717440 estimator.py:1147] Done calling model_fn.\n",
      "I0729 14:28:26.271385 139729986717440 monitored_session.py:240] Graph was finalized.\n",
      "I0729 14:28:26.278640 139729986717440 saver.py:1280] Restoring parameters from taxi_trained/model.ckpt-500\n",
      "I0729 14:28:26.336723 139729986717440 session_manager.py:500] Running local_init_op.\n",
      "I0729 14:28:26.346451 139729986717440 session_manager.py:502] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': array([9.537243], dtype=float32)}\n",
      "{'predictions': array([9.708608], dtype=float32)}\n",
      "{'predictions': array([9.596281], dtype=float32)}\n",
      "{'predictions': array([9.537495], dtype=float32)}\n",
      "{'predictions': array([9.537642], dtype=float32)}\n",
      "{'predictions': array([9.879642], dtype=float32)}\n",
      "{'predictions': array([9.879233], dtype=float32)}\n",
      "{'predictions': array([9.70439], dtype=float32)}\n",
      "{'predictions': array([9.535185], dtype=float32)}\n",
      "{'predictions': array([9.877537], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(input_fn = lambda: predict_input_fn(df = df_test[:10]))\n",
    "for items in predictions:\n",
    "    print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further evidence of the primitiveness of our model, it predicts similar amounts for every trip!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Estimator type\n",
    "\n",
    "One of the payoffs for using the Estimator API is we can swap in a different model type with just a few lines of code. Let's try a DNN. Note how now we need to specify the number of neurons in each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0729 14:28:26.529272 139729986717440 estimator.py:209] Using config: {'_num_ps_replicas': 0, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_experimental_distribute': None, '_task_type': 'worker', '_tf_random_seed': 1, '_master': '', '_is_chief': True, '_save_summary_steps': 100, '_task_id': 0, '_model_dir': 'taxi_trained', '_experimental_max_worker_delay_secs': None, '_service': None, '_eval_distribute': None, '_device_fn': None, '_keep_checkpoint_max': 5, '_global_id_in_cluster': 0, '_num_worker_replicas': 1, '_log_step_count_steps': 100, '_train_distribute': None, '_save_checkpoints_secs': 600, '_protocol': None, '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1560208c50>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000}\n",
      "I0729 14:28:26.580809 139729986717440 estimator.py:1145] Calling model_fn.\n",
      "W0729 14:28:26.585566 139729986717440 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0729 14:28:27.587177 139729986717440 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "I0729 14:28:27.634838 139729986717440 estimator.py:1147] Done calling model_fn.\n",
      "I0729 14:28:27.636195 139729986717440 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I0729 14:28:27.966065 139729986717440 monitored_session.py:240] Graph was finalized.\n",
      "I0729 14:28:28.055924 139729986717440 session_manager.py:500] Running local_init_op.\n",
      "I0729 14:28:28.064902 139729986717440 session_manager.py:502] Done running local_init_op.\n",
      "I0729 14:28:29.757470 139729986717440 basic_session_run_hooks.py:606] Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
      "I0729 14:28:30.327803 139729986717440 basic_session_run_hooks.py:262] loss = 881159.5, step = 1\n",
      "I0729 14:28:30.913655 139729986717440 basic_session_run_hooks.py:692] global_step/sec: 170.557\n",
      "I0729 14:28:30.916449 139729986717440 basic_session_run_hooks.py:260] loss = 6892.8047, step = 101 (0.589 sec)\n",
      "I0729 14:28:31.263861 139729986717440 basic_session_run_hooks.py:692] global_step/sec: 285.556\n",
      "I0729 14:28:31.266242 139729986717440 basic_session_run_hooks.py:260] loss = 14095.1, step = 201 (0.350 sec)\n",
      "I0729 14:28:31.615552 139729986717440 basic_session_run_hooks.py:692] global_step/sec: 284.328\n",
      "I0729 14:28:31.625307 139729986717440 basic_session_run_hooks.py:260] loss = 8984.71, step = 301 (0.359 sec)\n",
      "I0729 14:28:31.956652 139729986717440 basic_session_run_hooks.py:692] global_step/sec: 293.126\n",
      "I0729 14:28:31.958722 139729986717440 basic_session_run_hooks.py:260] loss = 19891.086, step = 401 (0.333 sec)\n",
      "I0729 14:28:32.251331 139729986717440 basic_session_run_hooks.py:606] Saving checkpoints for 500 into taxi_trained/model.ckpt.\n",
      "I0729 14:28:32.370526 139729986717440 estimator.py:368] Loss for final step: 13563.224.\n",
      "I0729 14:28:32.416176 139729986717440 estimator.py:1145] Calling model_fn.\n",
      "I0729 14:28:32.742618 139729986717440 estimator.py:1147] Done calling model_fn.\n",
      "I0729 14:28:32.766488 139729986717440 evaluation.py:255] Starting evaluation at 2019-07-29T14:28:32Z\n",
      "I0729 14:28:32.882021 139729986717440 monitored_session.py:240] Graph was finalized.\n",
      "I0729 14:28:32.887359 139729986717440 saver.py:1280] Restoring parameters from taxi_trained/model.ckpt-500\n",
      "I0729 14:28:32.947765 139729986717440 session_manager.py:500] Running local_init_op.\n",
      "I0729 14:28:32.978626 139729986717440 session_manager.py:502] Done running local_init_op.\n",
      "I0729 14:28:33.830934 139729986717440 evaluation.py:275] Finished evaluation at 2019-07-29-14:28:33\n",
      "I0729 14:28:33.832768 139729986717440 estimator.py:2039] Saving dict for global step 500: average_loss = 85.62229, global_step = 500, label/mean = 11.229713, loss = 10941.772, prediction/mean = 11.175714\n",
      "I0729 14:28:33.839056 139729986717440 estimator.py:2099] Saving 'checkpoint_path' summary for global step 500: taxi_trained/model.ckpt-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on dataset = 9.2532314120496\n",
      "CPU times: user 10.6 s, sys: 2.67 s, total: 13.3 s\n",
      "Wall time: 7.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True) \n",
    "model = tf.estimator.DNNRegressor(\n",
    "    hidden_units = [10,10], # specify neural architecture\n",
    "    feature_columns = feature_columns, \n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(tf_random_seed = 1)\n",
    ")\n",
    "model.train(\n",
    "    input_fn = lambda: train_input_fn(df = df_train), \n",
    "    steps = 500)\n",
    "print_rmse(model = model, df = df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our performance is only slightly better at 9.25, and still far worse than our rules based model.  This illustrates an important tenant of machine learning: A more complex model can't outrun bad data. \n",
    "\n",
    "Currently since we're not doing any feature engineering our input data has very little signal to learn from, so using a DNN doesn't help much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results summary\n",
    "\n",
    "We can summarize our results in a table here.\n",
    "   \n",
    "|Model | RMSE on validation set|\n",
    "|------|-----------------|\n",
    "|Rules Based Benchmark| 7.76|\n",
    "|Linear Model| 9.45|\n",
    "|DNN Model|9.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge exercise\n",
    "\n",
    "Create a neural network that is capable of finding the volume of a cylinder given the radius of its base (r) and its height (h). Assume that the radius and height of the cylinder are both in the range 0.5 to 2.0. Simulate the necessary training dataset.\n",
    "<p>\n",
    "Hint (highlight to see):\n",
    "<p style='color:white'>\n",
    "The input features will be r and h and the label will be $\\pi r^2 h$\n",
    "Create random values for r and h and compute V.\n",
    "Your dataset will consist of r, h and V.\n",
    "Then, use a DNN regressor.\n",
    "Make sure to generate enough data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
